{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1-4\n",
    "### Objective\n",
    "\n",
    "#### To determine whether a neural network can predict the population genetic parameter theta (θ) from simulated genetic data using a simple binning approach. This experiment serves as an initial step to assess the feasibility of our methodology before exploring more complex models.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "#### Data Simulation\n",
    "\n",
    "\t•\tSimulation Tool: msprime library for simulating ancestral histories and mutations.\n",
    "\t•\tNumber of Simulations: 10,000 datasets.\n",
    "\t•\tPopulation Parameters:\n",
    "\t•\tEffective Population Size ($N$): Fixed at 10,000 individuals.\n",
    "\t•\tMutation Rates ($\\mu$):\n",
    "\t•\tVaried across simulations.\n",
    "\t•\tGenerated 20 mutation rates logarithmically spaced between $1 \\times 10^{-9}$ and $5 \\times 10^{-8}$.\n",
    "\t•\tTheta Calculation:\n",
    "\t•\tCalculated for each simulation using the formula: $\\theta = 4N\\mu$.\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "\t•\tGenomic Sequence:\n",
    "\t•\tTotal length: 50,000 base pairs.\n",
    "\t•\tDivided into 50 bins, each 1,000 base pairs long.\n",
    "\t•\tMutation Counting:\n",
    "\t•\tCounted the number of mutations within each bin for each simulation.\n",
    "\t•\tData Normalization:\n",
    "\t•\tNormalized mutation counts within each simulation to scale between 0 and 1.\n",
    "\t•\tEnsured that mutation counts are comparable across different simulations.\n",
    "\n",
    "#### Neural Network Model\n",
    "\n",
    "\t•\tArchitecture:\n",
    "\t•\tInput Layer: Accepts normalized mutation counts per bin, shaped as (50 bins, 1 feature per bin).\n",
    "\t•\tConvolutional Layers:\n",
    "\t•\tConv1D Layer 1: 32 filters, kernel size of 3, ReLU activation.\n",
    "\t•\tConv1D Layer 2: 64 filters, kernel size of 3, ReLU activation.\n",
    "\t•\tFlatten Layer: Converts the output from convolutional layers into a 1D vector.\n",
    "\t•\tDense Layers:\n",
    "\t•\tDense Layer: 64 neurons, ReLU activation.\n",
    "\t•\tOutput Layer: 1 neuron for regression output (predicting $\\theta$).\n",
    "\t•\tCompilation:\n",
    "\t•\tOptimizer: Adam optimizer.\n",
    "\t•\tLoss Function: Mean Squared Error (MSE).\n",
    "\t•\tMetrics: Mean Absolute Error (MAE).\n",
    "\t•\tTraining Parameters:\n",
    "\t•\tEpochs: Maximum of 50 epochs.\n",
    "\t•\tBatch Size: 32 samples per batch.\n",
    "\t•\tEarly Stopping:\n",
    "\t•\tMonitors validation loss.\n",
    "\t•\tStops training if no improvement after 5 epochs.\n",
    "\t•\tRestores the best model weights.\n",
    "\n",
    "#### Data Splitting\n",
    "\n",
    "\t•\tTraining Set: 70% of the data.\n",
    "\t•\tValidation Set: 15% of the data.\n",
    "\t•\tTest Set: 15% of the data.\n",
    "\t•\tData Shuffling: Ensures random distribution of samples across sets.\n",
    "\n",
    "#### Note\n",
    "\tThe main purpose of Experiment 1 was to test the feasibality of our research goal. In order to achieve this, we constructed simple-straight forward models and evaluated their performance. Since we were trying things out, there are a couple of other models that has not been mentioned in the report. These approaches will be explained briefly, but will not be explained into depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Initial Prototype - Binning approach with only 1 feature(Number of Mutations)\n",
    "Key Characteristics:\n",
    "\n",
    "#### Simulation Parameters\n",
    "\n",
    "\t•\tPopulation Size (N): Fixed at 10,000.\n",
    "\t•\tMutation Rate (μ): Varied across simulations, chosen from 20 logarithmically spaced values between $1 \\times 10^{-9}$ and $5 \\times 10^{-8}$.\n",
    "\t•\tTheta Calculation: $\\theta = 4N\\mu$.\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "\t•\tMutation Counts per Bin:\n",
    "\t•\tThe genomic sequence is divided into 50 bins (each 1,000 base pairs).\n",
    "\t•\tCounts the number of mutations in each bin.\n",
    "\t•\tMutation counts are normalized within each simulation (scaled between 0 and 1).\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "\t•\tConvolutional Neural Network (CNN): Uses Conv1D layers.\n",
    "\t•\tInput Shape: (50 bins, 1 feature per bin).\n",
    "\t•\tTwo Conv1D layers followed by a Flatten layer and Dense layers.\n",
    "\t•\tPredicts $\\theta$ from the normalized mutation counts per bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/10000 datasets\n",
      "Processed 1000/10000 datasets\n",
      "Processed 1500/10000 datasets\n",
      "Processed 2000/10000 datasets\n",
      "Processed 2500/10000 datasets\n",
      "Processed 3000/10000 datasets\n",
      "Processed 3500/10000 datasets\n",
      "Processed 4000/10000 datasets\n",
      "Processed 4500/10000 datasets\n",
      "Processed 5000/10000 datasets\n",
      "Processed 5500/10000 datasets\n",
      "Processed 6000/10000 datasets\n",
      "Processed 6500/10000 datasets\n",
      "Processed 7000/10000 datasets\n",
      "Processed 7500/10000 datasets\n",
      "Processed 8000/10000 datasets\n",
      "Processed 8500/10000 datasets\n",
      "Processed 9000/10000 datasets\n",
      "Processed 9500/10000 datasets\n",
      "Processed 10000/10000 datasets\n",
      "Training samples: 7000\n",
      "Validation samples: 1500\n",
      "Test samples: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-12-05 18:21:45.565858: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-05 18:21:45.565888: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-05 18:21:45.565902: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-05 18:21:45.566090: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-05 18:21:45.566100: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2944</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">188,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2944\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m188,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">194,881</span> (761.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m194,881\u001b[0m (761.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">194,881</span> (761.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m194,881\u001b[0m (761.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 18:21:46.096294: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0034 - mae: 0.0241 - val_loss: 6.5456e-07 - val_mae: 5.0890e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 5.1827e-07 - mae: 5.0141e-04 - val_loss: 4.3836e-07 - val_mae: 4.7101e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 3.4460e-07 - mae: 4.6459e-04 - val_loss: 3.9807e-07 - val_mae: 4.6376e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 3.2076e-07 - mae: 4.5772e-04 - val_loss: 3.7472e-07 - val_mae: 4.6830e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.3128e-07 - mae: 4.6634e-04 - val_loss: 3.5950e-07 - val_mae: 4.6004e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.1915e-07 - mae: 4.5814e-04 - val_loss: 3.5296e-07 - val_mae: 4.6029e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.1572e-07 - mae: 4.5190e-04 - val_loss: 3.5213e-07 - val_mae: 4.5368e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.0679e-07 - mae: 4.4478e-04 - val_loss: 3.4387e-07 - val_mae: 4.6835e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m145/219\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.8532e-07 - mae: 4.2872e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m    133\u001b[0m test_loss, test_mae \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Daata preparation, model definition, training, and evaluation using\n",
    "# 10,000 data points\n",
    "# Feature -- 1: Number of Mutations\n",
    "# Binning Approach\n",
    "\n",
    "import msprime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Simulation constants\n",
    "NUM_SAMPLES = 50           # Number of diploid samples per simulation\n",
    "SEQUENCE_LENGTH = 50_000   # Length of the genomic region in base pairs\n",
    "BIN_SIZE = 1_000           # Bin size in base pairs\n",
    "NUM_BINS = SEQUENCE_LENGTH // BIN_SIZE  # Total number of bins\n",
    "POPULATION_SIZE = 10_000   # Effective population size (N)\n",
    "\n",
    "# Data generation parameters\n",
    "NUM_DATASETS = 10_000      # Total number of datasets to simulate\n",
    "\n",
    "# Generate a range of mutation rates (μ)\n",
    "MIN_MU = 1e-9\n",
    "MAX_MU = 5e-8\n",
    "\n",
    "# Generate mutation rates on a logarithmic scale\n",
    "mu_values = np.logspace(np.log10(MIN_MU), np.log10(MAX_MU), num=20)\n",
    "\n",
    "# Compute corresponding theta values\n",
    "theta_values_list = [4 * POPULATION_SIZE * mu for mu in mu_values]\n",
    "\n",
    "# Initialize lists to store inputs and labels\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Loop to generate NUM_DATASETS datasets\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Randomly select a mutation rate μ\n",
    "    mu = random.choice(mu_values)\n",
    "    theta = 4 * POPULATION_SIZE * mu\n",
    "\n",
    "    # Simulate ancestral history\n",
    "    ts = msprime.sim_ancestry(\n",
    "        samples=NUM_SAMPLES,\n",
    "        recombination_rate=0,\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        population_size=POPULATION_SIZE,\n",
    "        random_seed=random.randint(1, 1e6)\n",
    "    )\n",
    "\n",
    "    # Simulate mutations\n",
    "    mts = msprime.sim_mutations(ts, rate=mu, random_seed=random.randint(1, 1e6))\n",
    "\n",
    "    # Initialize mutation counts per bin\n",
    "    mutation_counts = np.zeros(NUM_BINS)\n",
    "\n",
    "    # Process mutations\n",
    "    for variant in mts.variants():\n",
    "        position = variant.site.position\n",
    "        bin_index = int(position // BIN_SIZE)\n",
    "        mutation_counts[bin_index] += 1\n",
    "\n",
    "    # Normalize mutation counts\n",
    "    max_count = mutation_counts.max()\n",
    "    if max_count > 0:\n",
    "        mutation_counts_normalized = mutation_counts / max_count\n",
    "    else:\n",
    "        mutation_counts_normalized = mutation_counts\n",
    "\n",
    "    # Reshape for CNN input\n",
    "    input_cnn = mutation_counts_normalized.reshape((NUM_BINS, 1))  # Shape: (NUM_BINS, 1)\n",
    "\n",
    "    # Append to lists\n",
    "    inputs.append(input_cnn)\n",
    "    labels.append(theta)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"Processed {i + 1}/{NUM_DATASETS} datasets\")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "inputs_array = np.array(inputs)      # Shape: (NUM_DATASETS, NUM_BINS, 1)\n",
    "labels_array = np.array(labels)      # Shape: (NUM_DATASETS,)\n",
    "\n",
    "# Shuffle and split the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    inputs_array, labels_array, test_size=0.15, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15 / 0.85, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Define the CNN model\n",
    "input_shape = (NUM_BINS, 1)  # (Number of bins, number of features per bin)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot predicted vs. true theta values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Diagonal line\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Adding features to prototype - Binning approach with 3 features(Number of Mutations) and varying population size & mutation rate\n",
    "Key Characteristics:\n",
    "\n",
    "#### Simulation Parameters:\n",
    "\t•\tPopulation Size (N): Varies across simulations.\n",
    "\t•\tRandomly chosen from 10 linearly spaced values between 5,000 and 20,000.\n",
    "\t•\tMutation Rate (μ): Varies across simulations.\n",
    "\t•\tRandomly chosen from 10 logarithmically spaced values between 1 \\times 10^{-9} and 5 \\times 10^{-8}.\n",
    "\t•\tTheta Calculation: \\theta = 4N\\mu.\n",
    "#### Feature Extraction:\n",
    "\t•\tAllele Frequency Spectrum (2D Histogram):\n",
    "\t•\tGenomic Bins: The genomic sequence is divided into bins (each 1,000 base pairs).\n",
    "\t•\tFrequency Bins: Allele frequencies are divided into 10 bins ranging from 0 to 1.\n",
    "\t•\t2D Histogram: For each variant, the count is incremented in the corresponding genomic bin and frequency bin, forming a 2D histogram.\n",
    "\t•\tAdditional Features per Genomic Bin:\n",
    "\t•\tWatterson’s Theta (θw): Calculated per genomic bin.\n",
    "\t•\tNucleotide Diversity (π): Calculated per genomic bin.\n",
    "\t•\tCombined Features:\n",
    "\t•\tThe 2D histogram and additional features are concatenated to form a combined feature matrix per simulation.\n",
    "#### Data Normalization:\n",
    "\t•\tHistogram Normalization: Counts in the histogram are normalized per genomic bin.\n",
    "\t•\tGlobal Normalization of Additional Features:\n",
    "\t•\tθw and π are normalized across all simulations to ensure consistency.\n",
    "#### Model Architecture:\n",
    "\t•\tConvolutional Neural Network (CNN): Uses Conv2D layers.\n",
    "\t•\tInput shape: (number of genomic bins, number of frequency bins + 2, 1).\n",
    "\t•\tTwo Conv2D layers followed by a Flatten layer and Dense layers.\n",
    "\t•\tPredicts θ from the combined features.\n",
    "#### Data Shapes:\n",
    "\t•\tInputs Array Shape: (number of datasets, number of genomic bins, number of frequency bins + 2, 1).\n",
    "#### Additional Analysis:\n",
    "\t•\tPopulation Size and Mutation Rate in Test Results:\n",
    "\t•\tThe test results include N and μ for each sample.\n",
    "#### Visualization:\n",
    "\t•\tPlots of predicted vs. true θ values, colored by population size or mutation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc\n",
    "import allel  # scikit-allel library\n",
    "import pandas as pd\n",
    "\n",
    "# Simulation constants\n",
    "NUM_SAMPLES_PER_SIM = 50      # Number of diploid samples per simulation\n",
    "SEQUENCE_LENGTH = 50_000      # Length of the genomic region in base pairs\n",
    "BIN_SIZE = 1_000              # Bin size in base pairs\n",
    "NUM_GENOMIC_BINS = SEQUENCE_LENGTH // BIN_SIZE  # Total number of genomic bins\n",
    "\n",
    "# Frequency histogram parameters\n",
    "NUM_FREQUENCY_BINS = 10       # Number of frequency bins\n",
    "FREQUENCY_BIN_EDGES = np.linspace(0, 1, NUM_FREQUENCY_BINS + 1)  # Edges from 0 to 1 inclusive\n",
    "\n",
    "# Data generation parameters\n",
    "NUM_DATASETS = 100000           # Total number of datasets to simulate (adjust as needed)\n",
    "BATCH_SIZE = 32               # Batch size for training\n",
    "\n",
    "# Generate a range of population sizes (N)\n",
    "MIN_N = 5_000\n",
    "MAX_N = 20_000\n",
    "N_values = np.linspace(MIN_N, MAX_N, num=10, dtype=int)  # 10 values from 5,000 to 20,000\n",
    "\n",
    "# Generate a range of mutation rates (μ)\n",
    "MIN_MU = 1e-9\n",
    "MAX_MU = 5e-8\n",
    "mu_values = np.logspace(np.log10(MIN_MU), np.log10(MAX_MU), num=10)  # 10 values on log scale\n",
    "\n",
    "# Initialize lists to store inputs and labels\n",
    "inputs = []\n",
    "labels = []\n",
    "theta_values = []        # To store theta values\n",
    "population_sizes = []    # To store N values\n",
    "mutation_rates = []      # To store μ values\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize lists for global normalization\n",
    "all_theta_w_bins = []\n",
    "all_pi_bins = []\n",
    "\n",
    "# Loop to generate NUM_DATASETS datasets\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Randomly select a population size N and mutation rate μ\n",
    "    N = random.choice(N_values)\n",
    "    mu = random.choice(mu_values)\n",
    "    theta = 4 * N * mu  # θ = 4Nμ\n",
    "\n",
    "    # Simulate ancestral history\n",
    "    ts = msprime.sim_ancestry(\n",
    "        samples=NUM_SAMPLES_PER_SIM,\n",
    "        recombination_rate=0,\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        population_size=N,\n",
    "        random_seed=random.randint(1, 1e6)\n",
    "    )\n",
    "\n",
    "    # Simulate mutations\n",
    "    mts = msprime.sim_mutations(ts, rate=mu, random_seed=random.randint(1, 1e6))\n",
    "\n",
    "    # Initialize the input matrix for this sample\n",
    "    input_matrix = np.zeros((NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS), dtype=np.float32)\n",
    "\n",
    "    # Initialize arrays for additional features\n",
    "    theta_w_bins = np.zeros(NUM_GENOMIC_BINS, dtype=np.float32)\n",
    "    pi_bins = np.zeros(NUM_GENOMIC_BINS, dtype=np.float32)\n",
    "\n",
    "    # Collect genotype data and positions\n",
    "    genotypes_list = []\n",
    "    positions_list = []\n",
    "\n",
    "    # Total number of alleles (diploid samples)\n",
    "    NUM_ALLELES = ts.num_samples  # Each sample represents one chromosome\n",
    "\n",
    "    for variant in mts.variants():\n",
    "        position = variant.site.position\n",
    "        genomic_bin_index = int(position // BIN_SIZE)\n",
    "\n",
    "        # Compute frequency\n",
    "        genotypes = variant.genotypes  # Array of 0s and 1s\n",
    "        derived_allele_count = np.count_nonzero(genotypes)  # Number of '1's\n",
    "        frequency = derived_allele_count / NUM_ALLELES  # Allele frequency\n",
    "\n",
    "        # Determine frequency bin\n",
    "        freq_bin_index = np.digitize(frequency, FREQUENCY_BIN_EDGES) - 1\n",
    "        freq_bin_index = min(freq_bin_index, NUM_FREQUENCY_BINS - 1)  # Ensure index is within bounds\n",
    "\n",
    "        # Increment the count\n",
    "        input_matrix[genomic_bin_index, freq_bin_index] += 1\n",
    "\n",
    "        # Collect data for additional features\n",
    "        genotypes_list.append(genotypes)\n",
    "        positions_list.append(position)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    if genotypes_list:\n",
    "        genotypes_array = np.vstack(genotypes_list)  # Shape: (num_variants, NUM_ALLELES)\n",
    "        positions_array = np.array(positions_list)\n",
    "    else:\n",
    "        genotypes_array = np.empty((0, NUM_ALLELES), dtype=np.int8)\n",
    "        positions_array = np.empty(0)\n",
    "\n",
    "    # Process each genomic bin for additional features\n",
    "    for bin_index in range(NUM_GENOMIC_BINS):\n",
    "        start = bin_index * BIN_SIZE\n",
    "        end = start + BIN_SIZE\n",
    "        in_bin = (positions_array >= start) & (positions_array < end)\n",
    "        genotypes_bin = genotypes_array[in_bin]\n",
    "\n",
    "        if genotypes_bin.size > 0:\n",
    "            # Convert genotypes to allele counts per variant\n",
    "            allele_counts = genotypes_bin.sum(axis=1)\n",
    "\n",
    "            # Number of segregating sites (S)\n",
    "            S = allele_counts.size\n",
    "\n",
    "            # Watterson's Theta (θw)\n",
    "            a1 = np.sum(1.0 / np.arange(1, NUM_ALLELES))\n",
    "            theta_w = S / a1 if a1 > 0 else 0.0\n",
    "            theta_w_bins[bin_index] = theta_w\n",
    "\n",
    "            # Nucleotide Diversity (π)\n",
    "            p = allele_counts / NUM_ALLELES\n",
    "            pi = 2 * np.sum(p * (1 - p))\n",
    "            pi_bins[bin_index] = pi\n",
    "        else:\n",
    "            theta_w_bins[bin_index] = 0.0\n",
    "            pi_bins[bin_index] = 0.0\n",
    "\n",
    "    # Collect theta_w_bins and pi_bins for global normalization\n",
    "    all_theta_w_bins.append(theta_w_bins)\n",
    "    all_pi_bins.append(pi_bins)\n",
    "\n",
    "    # Normalize histograms per genomic bin\n",
    "    bin_sums = input_matrix.sum(axis=1, keepdims=True)\n",
    "    input_matrix_normalized = input_matrix / bin_sums\n",
    "    input_matrix_normalized = np.nan_to_num(input_matrix_normalized)  # Replace NaNs with zero\n",
    "\n",
    "    # We will normalize additional features later\n",
    "\n",
    "    # Combine features (without normalizing additional features yet)\n",
    "    additional_features = np.stack([theta_w_bins, pi_bins], axis=1)  # Shape: (NUM_GENOMIC_BINS, 2)\n",
    "    combined_features = np.concatenate([input_matrix_normalized, additional_features], axis=1)  # Shape: (NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2)\n",
    "\n",
    "    # Expand dimensions to add channel for CNN input\n",
    "    input_cnn = combined_features[..., np.newaxis]  # Shape: (NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2, 1)\n",
    "\n",
    "    # Append to lists\n",
    "    inputs.append(input_cnn)\n",
    "    labels.append(theta)\n",
    "    theta_values.append(theta)\n",
    "    population_sizes.append(N)\n",
    "    mutation_rates.append(mu)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{NUM_DATASETS} datasets\")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "inputs_array = np.array(inputs, dtype=np.float32)    # Shape: (NUM_DATASETS, NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2, 1)\n",
    "labels_array = np.array(labels, dtype=np.float32)    # Shape: (NUM_DATASETS,)\n",
    "population_sizes_array = np.array(population_sizes, dtype=np.int32)\n",
    "mutation_rates_array = np.array(mutation_rates, dtype=np.float32)\n",
    "\n",
    "# Clean up\n",
    "del inputs\n",
    "del labels\n",
    "gc.collect()\n",
    "\n",
    "# Global normalization of additional features\n",
    "all_theta_w_bins_array = np.concatenate(all_theta_w_bins)\n",
    "all_pi_bins_array = np.concatenate(all_pi_bins)\n",
    "\n",
    "global_max_theta_w = np.max(all_theta_w_bins_array)\n",
    "global_max_pi = np.max(all_pi_bins_array)\n",
    "\n",
    "# Avoid division by zero\n",
    "global_max_theta_w = global_max_theta_w if global_max_theta_w > 0 else 1\n",
    "global_max_pi = global_max_pi if global_max_pi > 0 else 1\n",
    "\n",
    "# Normalize additional features across the entire dataset\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Normalize theta_w_bins and pi_bins in inputs_array\n",
    "    inputs_array[i, :, NUM_FREQUENCY_BINS, 0] /= global_max_theta_w\n",
    "    inputs_array[i, :, NUM_FREQUENCY_BINS + 1, 0] /= global_max_pi\n",
    "\n",
    "# Shuffle and split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test, N_train_val, N_test = train_test_split(\n",
    "    inputs_array, labels_array, population_sizes_array, test_size=0.15, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val, N_train, N_val = train_test_split(\n",
    "    X_train_val, y_train_val, N_train_val, test_size=0.15 / 0.85, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Clean up\n",
    "del inputs_array\n",
    "del labels_array\n",
    "del population_sizes_array\n",
    "gc.collect()\n",
    "\n",
    "# Define the CNN model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "input_shape = (NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2, 1)  # (Height, Width, Channels)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss (MSE): {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Optional: Include population sizes and mutation rates for the test set\n",
    "N_test = N_test[:len(y_test)]\n",
    "mu_test = mutation_rates_array[-len(y_test):]\n",
    "\n",
    "# Create a DataFrame with test results\n",
    "test_results = pd.DataFrame({\n",
    "    'True Theta': y_test,\n",
    "    'Predicted Theta': y_pred,\n",
    "    'Population Size': N_test,\n",
    "    'Mutation Rate': mu_test\n",
    "})\n",
    "\n",
    "# Plot predicted vs. true theta values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_results['True Theta'], test_results['Predicted Theta'], alpha=0.5)\n",
    "plt.plot([test_results['True Theta'].min(), test_results['True Theta'].max()],\n",
    "         [test_results['True Theta'].min(), test_results['True Theta'].max()], 'r--')\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values')\n",
    "plt.show()\n",
    "\n",
    "# Optional Analysis: Plot predictions colored by Population Size\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(test_results['True Theta'], test_results['Predicted Theta'],\n",
    "                      c=test_results['Population Size'], cmap='viridis', alpha=0.7)\n",
    "plt.plot([test_results['True Theta'].min(), test_results['True Theta'].max()],\n",
    "         [test_results['True Theta'].min(), test_results['True Theta'].max()], 'r--')\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values (Colored by Population Size)')\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Population Size')\n",
    "plt.show()\n",
    "\n",
    "# Optional Analysis: Plot predictions colored by Mutation Rate\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(test_results['True Theta'], test_results['Predicted Theta'],\n",
    "                      c=test_results['Mutation Rate'], cmap='plasma', alpha=0.7)\n",
    "plt.plot([test_results['True Theta'].min(), test_results['True Theta'].max()],\n",
    "         [test_results['True Theta'].min(), test_results['True Theta'].max()], 'r--')\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values (Colored by Mutation Rate)')\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Mutation Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Additional feature - Including Allele Frequency Spectrum\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "#### Simulation Parameters\n",
    "\n",
    "\t•\tPopulation Size (N): Varies across simulations.\n",
    "\t•\tRandomly selected from 10 linearly spaced values between 5,000 and 20,000.\n",
    "\t•\tMutation Rate (μ): Varies across simulations.\n",
    "\t•\tRandomly selected from 10 logarithmically spaced values between 1 \\times 10^{-9} and 5 \\times 10^{-8}.\n",
    "\t•\tTheta Calculation: \\theta = 4N\\mu.\n",
    "\t•\tNumber of Simulations: 100,000 datasets.\n",
    "\t•\tSamples per Simulation: 50 diploid individuals (100 chromosomes).\n",
    "\t•\tSequence Length: 50,000 base pairs.\n",
    "\t•\tBin Size: 1,000 base pairs, resulting in 50 genomic bins.\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "##### Allele Frequency Spectrum (2D Histogram)\n",
    "\n",
    "\t•\tGenomic Bins:\n",
    "\t•\tThe genomic sequence is divided into 50 bins, each 1,000 base pairs long.\n",
    "\t•\tFrequency Bins:\n",
    "\t•\tAllele frequencies are divided into 10 bins ranging from 0 to 1.\n",
    "\t•\tFrequency bin edges are defined using np.linspace(0, 1, NUM_FREQUENCY_BINS + 1).\n",
    "\t•\t2D Histogram:\n",
    "\t•\tFor each variant (mutation), counts are incremented in the corresponding genomic bin and frequency bin, forming a 2D histogram.\n",
    "\t•\tThis results in an input matrix of shape (50 genomic bins, 10 frequency bins).\n",
    "\n",
    "##### Additional Features per Genomic Bin\n",
    "\n",
    "\t•\tWatterson’s Theta (θw):\n",
    "\t•\tCalculated per genomic bin using the number of segregating sites (S) and the harmonic sum\n",
    "\t•\tNucleotide Diversity (π):\n",
    "\t•\tCalculated per genomic bin based on allele frequencies.\n",
    "\n",
    "#### Data Normalization\n",
    "\t•\tHistogram Normalization:\n",
    "\t•\tCounts in the histogram are normalized per genomic bin by dividing each frequency bin count by the total count of mutations in that genomic bin.\n",
    "\t•\tThis handles varying mutation counts across bins and simulations.\n",
    "\t•\tNaN values resulting from division by zero are replaced with zeros.\n",
    "\t•\tGlobal Normalization of Additional Features:\n",
    "\t•\tθw and π are concatenated across all datasets to find the global maximum values.\n",
    "\t•\tEach θw and π value is then divided by its global maximum to scale these features between 0 and 1.\n",
    "\t•\tThis ensures consistency across the entire dataset.\n",
    "\n",
    "#### Combined Features\n",
    "\t•\tFeature Matrix:\n",
    "\t•\tThe normalized histogram and the normalized additional features (θw and π) are concatenated along the feature axis.\n",
    "\t•\tResults in a combined feature matrix of shape (50 genomic bins, 12 features per bin), where:\n",
    "\t•\t10 features from frequency bins.\n",
    "\t•\t2 features from θw and π.\n",
    "\t•\tInput for the Model:\n",
    "\t•\tThe combined feature matrix is used as input to the model without adding extra dimensions, suitable for RNN input.\n",
    "\t•\tInput Shape: (50 genomic bins, 12 features per bin).\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "Recurrent Neural Network (RNN) with LSTM Layer\n",
    "\n",
    "\t•\tInput Layer:\n",
    "\t•\tAccepts a sequence of genomic bins, each represented by a 12-dimensional feature vector.\n",
    "\t•\tInput Shape: (50 genomic bins, 12 features per bin).\n",
    "\t•\tLSTM Layer:\n",
    "\t•\tUnits: 64.\n",
    "\t•\tFunction: Processes the sequential data to capture dependencies and patterns across genomic bins.\n",
    "\t•\tReturn Sequences: False (only the final output is used).\n",
    "\t•\tDense Layers:\n",
    "\t•\tHidden Dense Layer:\n",
    "\t•\t64 neurons with ReLU activation.\n",
    "\t•\tOutput Layer:\n",
    "\t•\t1 neuron for regression output (predicting θ).\n",
    "\n",
    "Compilation\n",
    "\n",
    "\t•\tOptimizer: Adam optimizer.\n",
    "\t•\tLoss Function: Mean Squared Error (MSE), suitable for regression tasks.\n",
    "\t•\tMetrics: Mean Absolute Error (MAE) for additional performance evaluation.\n",
    "\n",
    "Training Parameters\n",
    "\n",
    "\t•\tEpochs: Maximum of 50 epochs.\n",
    "\t•\tBatch Size: 32 samples per batch.\n",
    "\t•\tEarly Stopping:\n",
    "\t•\tMonitors validation loss.\n",
    "\t•\tStops training if no improvement after 5 epochs (patience).\n",
    "\t•\tRestores the best model weights to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc\n",
    "import allel  # scikit-allel library\n",
    "import pandas as pd\n",
    "\n",
    "# Simulation constants\n",
    "NUM_SAMPLES_PER_SIM = 50      # Number of diploid samples per simulation\n",
    "SEQUENCE_LENGTH = 50_000      # Length of the genomic region in base pairs\n",
    "BIN_SIZE = 1_000              # Bin size in base pairs\n",
    "NUM_GENOMIC_BINS = SEQUENCE_LENGTH // BIN_SIZE  # Total number of genomic bins\n",
    "\n",
    "# Frequency histogram parameters\n",
    "NUM_FREQUENCY_BINS = 10       # Number of frequency bins\n",
    "FREQUENCY_BIN_EDGES = np.linspace(0, 1, NUM_FREQUENCY_BINS + 1)  # Edges from 0 to 1 inclusive\n",
    "\n",
    "# Data generation parameters\n",
    "NUM_DATASETS = 100000           # Total number of datasets to simulate (adjust as needed)\n",
    "BATCH_SIZE = 32               # Batch size for training\n",
    "\n",
    "# Generate a range of population sizes (N)\n",
    "MIN_N = 5_000\n",
    "MAX_N = 20_000\n",
    "N_values = np.linspace(MIN_N, MAX_N, num=10, dtype=int)  # 10 values from 5,000 to 20,000\n",
    "\n",
    "# Generate a range of mutation rates (μ)\n",
    "MIN_MU = 1e-9\n",
    "MAX_MU = 5e-8\n",
    "mu_values = np.logspace(np.log10(MIN_MU), np.log10(MAX_MU), num=10)  # 10 values on log scale\n",
    "\n",
    "# Initialize lists to store inputs and labels\n",
    "inputs = []\n",
    "labels = []\n",
    "theta_values = []        # To store theta values\n",
    "population_sizes = []    # To store N values\n",
    "mutation_rates = []      # To store μ values\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize lists for global normalization\n",
    "all_theta_w_bins = []\n",
    "all_pi_bins = []\n",
    "\n",
    "# Loop to generate NUM_DATASETS datasets\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Randomly select a population size N and mutation rate μ\n",
    "    N = random.choice(N_values)\n",
    "    mu = random.choice(mu_values)\n",
    "    theta = 4 * N * mu  # θ = 4Nμ\n",
    "\n",
    "    # Simulate ancestral history\n",
    "    ts = msprime.sim_ancestry(\n",
    "        samples=NUM_SAMPLES_PER_SIM,\n",
    "        recombination_rate=0,\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        population_size=N,\n",
    "        random_seed=random.randint(1, 1e6)\n",
    "    )\n",
    "\n",
    "    # Simulate mutations\n",
    "    mts = msprime.sim_mutations(ts, rate=mu, random_seed=random.randint(1, 1e6))\n",
    "\n",
    "    # Initialize the input matrix for this sample\n",
    "    input_matrix = np.zeros((NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS), dtype=np.float32)\n",
    "\n",
    "    # Initialize arrays for additional features\n",
    "    theta_w_bins = np.zeros(NUM_GENOMIC_BINS, dtype=np.float32)\n",
    "    pi_bins = np.zeros(NUM_GENOMIC_BINS, dtype=np.float32)\n",
    "\n",
    "    # Collect genotype data and positions\n",
    "    genotypes_list = []\n",
    "    positions_list = []\n",
    "\n",
    "    # Total number of alleles (diploid samples)\n",
    "    NUM_ALLELES = ts.num_samples  # Each sample represents one chromosome\n",
    "\n",
    "    for variant in mts.variants():\n",
    "        position = variant.site.position\n",
    "        genomic_bin_index = int(position // BIN_SIZE)\n",
    "\n",
    "        # Compute frequency\n",
    "        genotypes = variant.genotypes  # Array of 0s and 1s\n",
    "        derived_allele_count = np.count_nonzero(genotypes)  # Number of '1's\n",
    "        frequency = derived_allele_count / NUM_ALLELES  # Allele frequency\n",
    "\n",
    "        # Determine frequency bin\n",
    "        freq_bin_index = np.digitize(frequency, FREQUENCY_BIN_EDGES) - 1\n",
    "        freq_bin_index = min(freq_bin_index, NUM_FREQUENCY_BINS - 1)  # Ensure index is within bounds\n",
    "\n",
    "        # Increment the count\n",
    "        input_matrix[genomic_bin_index, freq_bin_index] += 1\n",
    "\n",
    "        # Collect data for additional features\n",
    "        genotypes_list.append(genotypes)\n",
    "        positions_list.append(position)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    if genotypes_list:\n",
    "        genotypes_array = np.vstack(genotypes_list)  # Shape: (num_variants, NUM_ALLELES)\n",
    "        positions_array = np.array(positions_list)\n",
    "    else:\n",
    "        genotypes_array = np.empty((0, NUM_ALLELES), dtype=np.int8)\n",
    "        positions_array = np.empty(0)\n",
    "\n",
    "    # Process each genomic bin for additional features\n",
    "    for bin_index in range(NUM_GENOMIC_BINS):\n",
    "        start = bin_index * BIN_SIZE\n",
    "        end = start + BIN_SIZE\n",
    "        in_bin = (positions_array >= start) & (positions_array < end)\n",
    "        genotypes_bin = genotypes_array[in_bin]\n",
    "\n",
    "        if genotypes_bin.size > 0:\n",
    "            # Convert genotypes to allele counts per variant\n",
    "            allele_counts = genotypes_bin.sum(axis=1)\n",
    "\n",
    "            # Number of segregating sites (S)\n",
    "            S = allele_counts.size\n",
    "\n",
    "            # Watterson's Theta (θw)\n",
    "            a1 = np.sum(1.0 / np.arange(1, NUM_ALLELES))\n",
    "            theta_w = S / a1 if a1 > 0 else 0.0\n",
    "            theta_w_bins[bin_index] = theta_w\n",
    "\n",
    "            # Nucleotide Diversity (π)\n",
    "            p = allele_counts / NUM_ALLELES\n",
    "            pi = 2 * np.sum(p * (1 - p))\n",
    "            pi_bins[bin_index] = pi\n",
    "        else:\n",
    "            theta_w_bins[bin_index] = 0.0\n",
    "            pi_bins[bin_index] = 0.0\n",
    "\n",
    "    # Collect theta_w_bins and pi_bins for global normalization\n",
    "    all_theta_w_bins.append(theta_w_bins)\n",
    "    all_pi_bins.append(pi_bins)\n",
    "\n",
    "    # Normalize histograms per genomic bin\n",
    "    bin_sums = input_matrix.sum(axis=1, keepdims=True)\n",
    "    input_matrix_normalized = input_matrix / bin_sums\n",
    "    input_matrix_normalized = np.nan_to_num(input_matrix_normalized)  # Replace NaNs with zero\n",
    "\n",
    "    # We will normalize additional features later\n",
    "\n",
    "    # Combine features (without normalizing additional features yet)\n",
    "    additional_features = np.stack([theta_w_bins, pi_bins], axis=1)  # Shape: (NUM_GENOMIC_BINS, 2)\n",
    "    combined_features = np.concatenate([input_matrix_normalized, additional_features], axis=1)  # Shape: (NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2)\n",
    "\n",
    "    # No need to add channel dimension for RNNs\n",
    "    input_rnn = combined_features  # Shape: (NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2)\n",
    "\n",
    "    # Append to lists\n",
    "    inputs.append(input_rnn)\n",
    "    labels.append(theta)\n",
    "    theta_values.append(theta)\n",
    "    population_sizes.append(N)\n",
    "    mutation_rates.append(mu)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{NUM_DATASETS} datasets\")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "inputs_array = np.array(inputs, dtype=np.float32)    # Shape: (NUM_DATASETS, NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2)\n",
    "labels_array = np.array(labels, dtype=np.float32)    # Shape: (NUM_DATASETS,)\n",
    "population_sizes_array = np.array(population_sizes, dtype=np.int32)\n",
    "mutation_rates_array = np.array(mutation_rates, dtype=np.float32)\n",
    "\n",
    "# Clean up\n",
    "del inputs\n",
    "del labels\n",
    "gc.collect()\n",
    "\n",
    "# Global normalization of additional features\n",
    "all_theta_w_bins_array = np.concatenate(all_theta_w_bins)\n",
    "all_pi_bins_array = np.concatenate(all_pi_bins)\n",
    "\n",
    "global_max_theta_w = np.max(all_theta_w_bins_array)\n",
    "global_max_pi = np.max(all_pi_bins_array)\n",
    "\n",
    "# Avoid division by zero\n",
    "global_max_theta_w = global_max_theta_w if global_max_theta_w > 0 else 1\n",
    "global_max_pi = global_max_pi if global_max_pi > 0 else 1\n",
    "\n",
    "# Normalize additional features across the entire dataset\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Normalize theta_w_bins and pi_bins in inputs_array\n",
    "    inputs_array[i, :, NUM_FREQUENCY_BINS] /= global_max_theta_w\n",
    "    inputs_array[i, :, NUM_FREQUENCY_BINS + 1] /= global_max_pi\n",
    "\n",
    "# Shuffle and split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test, N_train_val, N_test = train_test_split(\n",
    "    inputs_array, labels_array, population_sizes_array, test_size=0.15, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val, N_train, N_val = train_test_split(\n",
    "    X_train_val, y_train_val, N_train_val, test_size=0.15 / 0.85, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Clean up\n",
    "del inputs_array\n",
    "del labels_array\n",
    "del population_sizes_array\n",
    "gc.collect()\n",
    "\n",
    "# Define the RNN model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "input_shape = (NUM_GENOMIC_BINS, NUM_FREQUENCY_BINS + 2)  # (Timesteps, Features)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(units=64, return_sequences=False, input_shape=input_shape),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss (MSE): {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Optional: Include population sizes and mutation rates for the test set\n",
    "N_test = N_test[:len(y_test)]\n",
    "mu_test = mutation_rates_array[-len(y_test):]\n",
    "\n",
    "# Create a DataFrame with test results\n",
    "test_results = pd.DataFrame({\n",
    "    'True Theta': y_test,\n",
    "    'Predicted Theta': y_pred,\n",
    "    'Population Size': N_test,\n",
    "    'Mutation Rate': mu_test\n",
    "})\n",
    "\n",
    "# Plot predicted vs. true theta values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_results['True Theta'], test_results['Predicted Theta'], alpha=0.5)\n",
    "plt.plot([test_results['True Theta'].min(), test_results['True Theta'].max()],\n",
    "         [test_results['True Theta'].min(), test_results['True Theta'].max()], 'r--')\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values')\n",
    "plt.show()\n",
    "\n",
    "# Optional Analysis: Plot predictions colored by Population Size\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(test_results['True Theta'], test_results['Predicted Theta'],\n",
    "                      c=test_results['Population Size'], cmap='viridis', alpha=0.7)\n",
    "plt.plot([test_results['True Theta'].min(), test_results['True Theta'].max()],\n",
    "         [test_results['True Theta'].min(), test_results['True Theta'].max()], 'r--')\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values (Colored by Population Size)')\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Population Size')\n",
    "plt.show()\n",
    "\n",
    "# Optional Analysis: Plot predictions colored by Mutation Rate\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(test_results['True Theta'], test_results['Predicted Theta'],\n",
    "                      c=test_results['Mutation Rate'], cmap='plasma', alpha=0.7)\n",
    "plt.plot([test_results['True Theta'].min(), test_results['True Theta'].max()],\n",
    "         [test_results['True Theta'].min(), test_results['True Theta'].max()], 'r--')\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values (Colored by Mutation Rate)')\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Mutation Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Same features as 3, but different model architecture\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "#### Simulation Parameters\n",
    "\n",
    "\t•\tPopulation Size (N): Varies across simulations.\n",
    "\t•\tRandomly selected from 10 linearly spaced values between 5,000 and 20,000.\n",
    "\t•\tMutation Rate (μ): Varies across simulations.\n",
    "\t•\tRandomly selected from 10 logarithmically spaced values between 1 \\times 10^{-9} and 5 \\times 10^{-8}.\n",
    "\t•\tTheta Calculation: \\theta = 4N\\mu.\n",
    "\t•\tNumber of Simulations: 1,000,000 datasets.\n",
    "\t•\tSamples per Simulation: 50 diploid individuals (100 chromosomes).\n",
    "\t•\tSequence Length: 50,000 base pairs.\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "##### Site Frequency Spectrum (SFS)\n",
    "\n",
    "\t•\tAllele Frequencies:\n",
    "\t•\tFor each mutation, the allele frequency is calculated as the proportion of derived alleles among all alleles.\n",
    "\t•\tAllele frequencies are collected across the entire sequence.\n",
    "\t•\tFrequency Bins:\n",
    "\t•\tAllele frequencies are divided into 20 bins ranging from 0 to 1.\n",
    "\t•\tFrequency bin edges are defined using np.linspace(0, 1, NUM_FREQUENCY_BINS + 1).\n",
    "\t•\tSFS Calculation:\n",
    "\t•\tA histogram of allele frequencies is created, resulting in the Site Frequency Spectrum.\n",
    "\t•\tThe SFS counts are normalized to sum to 1, forming a probability distribution.\n",
    "\t•\tFeature Vector: The normalized SFS is represented as a vector of length 20.\n",
    "\n",
    "##### Additional Global Features\n",
    "\n",
    "\t•\tWatterson’s Theta (θw):\n",
    "\t•\tCalculated over the entire sequence using the number of segregating sites (S) and the harmonic sum\n",
    "\n",
    "    •\tNucleotide Diversity (π):\n",
    "\t•\tCalculated over the entire sequence based on allele frequencies.\n",
    "\n",
    "    •\tFeature Combination:\n",
    "\t•\tThe additional features θw and π are appended to the normalized SFS vector.\n",
    "\t•\tTotal Feature Vector Length: 22 features (20 from SFS + θw + π).\n",
    "\n",
    "#### Data Normalization\n",
    "\t•\tGlobal Normalization of Additional Features:\n",
    "\t•\tθw and π values are collected across all datasets to find their global maximums.\n",
    "\t•\tEach θw and π value is then divided by its global maximum to scale these features between 0 and 1.\n",
    "\t•\tThis ensures consistency across the entire dataset.\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "##### Multilayer Perceptron (MLP)\n",
    "\n",
    "\t•\tInput Layer:\n",
    "\t•\tAccepts the combined feature vector for each simulation.\n",
    "\t•\tInput Shape: (22 features,)\n",
    "\t•\tHidden Layers:\n",
    "\t•\tFirst Dense Layer:\n",
    "\t•\t64 neurons with ReLU activation.\n",
    "\t•\tDropout Layer:\n",
    "\t•\tDropout rate of 0.2 to prevent overfitting.\n",
    "\t•\tSecond Dense Layer:\n",
    "\t•\t64 neurons with ReLU activation.\n",
    "\t•\tDropout Layer:\n",
    "\t•\tDropout rate of 0.2.\n",
    "\t•\tOutput Layer:\n",
    "\t•\t1 neuron for regression output (predicting θ).\n",
    "\n",
    "##### Compilation\n",
    "\n",
    "\t•\tOptimizer: Adam optimizer.\n",
    "\t•\tLoss Function: Mean Squared Error (MSE), suitable for regression tasks.\n",
    "\t•\tMetrics: Mean Absolute Error (MAE) for additional performance evaluation.\n",
    "\n",
    "##### Training Parameters\n",
    "\n",
    "\t•\tEpochs: Maximum of 100 epochs.\n",
    "\t•\tBatch Size: 32 samples per batch.\n",
    "\t•\tEarly Stopping:\n",
    "\t•\tMonitors validation loss.\n",
    "\t•\tStops training if no improvement after 5 epochs (patience).\n",
    "\t•\tRestores the best model weights to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc\n",
    "import allel  # scikit-allel library\n",
    "import pandas as pd\n",
    "\n",
    "# Simulation constants\n",
    "NUM_SAMPLES_PER_SIM = 50      # Number of diploid samples per simulation\n",
    "SEQUENCE_LENGTH = 50_000      # Length of the genomic region in base pairs\n",
    "\n",
    "# Frequency histogram parameters\n",
    "NUM_FREQUENCY_BINS = 20       # Number of frequency bins for the SFS\n",
    "FREQUENCY_BIN_EDGES = np.linspace(0, 1, NUM_FREQUENCY_BINS + 1)  # Edges from 0 to 1 inclusive\n",
    "\n",
    "# Data generation parameters\n",
    "NUM_DATASETS = 1000000           # Total number of datasets to simulate (adjust as needed)\n",
    "BATCH_SIZE = 32               # Batch size for training\n",
    "\n",
    "# Generate a range of population sizes (N)\n",
    "MIN_N = 5_000\n",
    "MAX_N = 20_000\n",
    "N_values = np.linspace(MIN_N, MAX_N, num=10, dtype=int)  # 10 values from 5,000 to 20,000\n",
    "\n",
    "# Generate a range of mutation rates (μ)\n",
    "MIN_MU = 1e-9\n",
    "MAX_MU = 5e-8\n",
    "mu_values = np.logspace(np.log10(MIN_MU), np.log10(MAX_MU), num=10)  # 10 values on log scale\n",
    "\n",
    "# Initialize lists to store inputs and labels\n",
    "inputs = []\n",
    "labels = []\n",
    "theta_values = []        # To store theta values\n",
    "population_sizes = []    # To store N values\n",
    "mutation_rates = []      # To store μ values\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize lists for global normalization\n",
    "all_theta_w = []\n",
    "all_pi = []\n",
    "\n",
    "# Loop to generate NUM_DATASETS datasets\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Randomly select a population size N and mutation rate μ\n",
    "    N = random.choice(N_values)\n",
    "    mu = random.choice(mu_values)\n",
    "    theta = 4 * N * mu  # θ = 4Nμ\n",
    "\n",
    "    # Simulate ancestral history\n",
    "    ts = msprime.sim_ancestry(\n",
    "        samples=NUM_SAMPLES_PER_SIM,\n",
    "        recombination_rate=0,\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        population_size=N,\n",
    "        random_seed=random.randint(1, 1e6)\n",
    "    )\n",
    "\n",
    "    # Simulate mutations\n",
    "    mts = msprime.sim_mutations(ts, rate=mu, random_seed=random.randint(1, 1e6))\n",
    "\n",
    "    # Collect allele frequencies\n",
    "    allele_frequencies = []\n",
    "\n",
    "    # Collect genotype data\n",
    "    genotypes_list = []\n",
    "    for variant in mts.variants():\n",
    "        genotypes = variant.genotypes  # Array of 0s and 1s\n",
    "        derived_allele_count = np.count_nonzero(genotypes)  # Number of '1's\n",
    "        frequency = derived_allele_count / mts.num_samples  # Allele frequency\n",
    "        allele_frequencies.append(frequency)\n",
    "\n",
    "        genotypes_list.append(genotypes)\n",
    "\n",
    "    # Convert allele frequencies to numpy array\n",
    "    allele_frequencies = np.array(allele_frequencies)\n",
    "\n",
    "    # Compute SFS (Site Frequency Spectrum)\n",
    "    sfs_counts, _ = np.histogram(allele_frequencies, bins=FREQUENCY_BIN_EDGES)\n",
    "    sfs_normalized = sfs_counts / sfs_counts.sum() if sfs_counts.sum() > 0 else sfs_counts  # Normalize SFS to sum to 1\n",
    "\n",
    "    # Compute Watterson's Theta (θw) over the entire sequence\n",
    "    S = len(allele_frequencies)  # Number of segregating sites\n",
    "    a1 = np.sum(1.0 / np.arange(1, mts.num_samples))\n",
    "    theta_w = S / a1 if a1 > 0 else 0.0\n",
    "\n",
    "    # Compute nucleotide diversity (π) over the entire sequence\n",
    "    if genotypes_list:\n",
    "        genotypes_array = np.vstack(genotypes_list)  # Shape: (num_variants, NUM_SAMPLES)\n",
    "        allele_counts = genotypes_array.sum(axis=1)\n",
    "        p = allele_counts / mts.num_samples\n",
    "        pi = 2 * np.sum(p * (1 - p))\n",
    "    else:\n",
    "        pi = 0.0\n",
    "\n",
    "    # Collect θw and π for global normalization\n",
    "    all_theta_w.append(theta_w)\n",
    "    all_pi.append(pi)\n",
    "\n",
    "    # Combine features\n",
    "    # Features: SFS + θw + π\n",
    "    additional_features = np.array([theta_w, pi], dtype=np.float32)\n",
    "    combined_features = np.concatenate([sfs_normalized, additional_features])\n",
    "\n",
    "    # Append to lists\n",
    "    inputs.append(combined_features)\n",
    "    labels.append(theta)\n",
    "    theta_values.append(theta)\n",
    "    population_sizes.append(N)\n",
    "    mutation_rates.append(mu)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{NUM_DATASETS} datasets\")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "inputs_array = np.array(inputs, dtype=np.float32)    # Shape: (NUM_DATASETS, NUM_FREQUENCY_BINS + 2)\n",
    "labels_array = np.array(labels, dtype=np.float32)    # Shape: (NUM_DATASETS,)\n",
    "population_sizes_array = np.array(population_sizes, dtype=np.int32)\n",
    "mutation_rates_array = np.array(mutation_rates, dtype=np.float32)\n",
    "\n",
    "# Clean up\n",
    "del inputs\n",
    "del labels\n",
    "gc.collect()\n",
    "\n",
    "# Convert lists to arrays\n",
    "all_theta_w_array = np.array(all_theta_w, dtype=np.float32)\n",
    "all_pi_array = np.array(all_pi, dtype=np.float32)\n",
    "\n",
    "# Global normalization of θw and π\n",
    "global_max_theta_w = np.max(all_theta_w_array)\n",
    "global_max_pi = np.max(all_pi_array)\n",
    "\n",
    "# Avoid division by zero\n",
    "global_max_theta_w = global_max_theta_w if global_max_theta_w > 0 else 1\n",
    "global_max_pi = global_max_pi if global_max_pi > 0 else 1\n",
    "\n",
    "# Normalize θw and π across the entire dataset\n",
    "inputs_array[:, -2] /= global_max_theta_w\n",
    "inputs_array[:, -1] /= global_max_pi\n",
    "\n",
    "# Shuffle and split the data\n",
    "X_train_val, X_test, y_train_val, y_test, N_train_val, N_test = train_test_split(\n",
    "    inputs_array, labels_array, population_sizes_array, test_size=0.15, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val, N_train, N_val = train_test_split(\n",
    "    X_train_val, y_train_val, N_train_val, test_size=0.15 / 0.85, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Clean up\n",
    "del inputs_array\n",
    "del labels_array\n",
    "gc.collect()\n",
    "\n",
    "# Define the MLP model\n",
    "input_shape = (NUM_FREQUENCY_BINS + 2,)  # Total number of features\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=input_shape),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss (MSE): {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Optional: Include population sizes and mutation rates for the test set\n",
    "N_test = N_test[:len(y_test)]\n",
    "mu_test = mutation_rates_array[-len(y_test):]\n",
    "\n",
    "# Create a DataFrame with test results\n",
    "test_results = pd.DataFrame({\n",
    "    'True Theta': y_test,\n",
    "    'Predicted Theta': y_pred,\n",
    "    'Population Size': N_test,\n",
    "    'Mutation Rate': mu_test\n",
    "})\n",
    "\n",
    "# Plot predicted vs. true theta values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_results['True Theta'], test_results['Predicted Theta'], alpha=0.5)\n",
    "plt.plot([test_results['True Theta'].min(), test_results['True Theta'].max()],\n",
    "         [test_results['True Theta'].min(), test_results['True Theta'].max()], 'r--')\n",
    "plt.xlabel('True Theta Values')\n",
    "plt.ylabel('Predicted Theta Values')\n",
    "plt.title('Predicted vs. True Theta Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
