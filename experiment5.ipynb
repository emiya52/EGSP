{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import msprime\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import skew, kurtosis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation constants\n",
    "NUM_SAMPLES_PER_SIM = 50      # Number of diploid samples per simulation\n",
    "SEQUENCE_LENGTH = 50_000      # Length of the genomic region in base pairs\n",
    "\n",
    "# Data generation parameters\n",
    "NUM_DATASETS = 10_000           # Total number of datasets to simulate\n",
    "BATCH_SIZE = 32               # Batch size for training\n",
    "\n",
    "# Adjusted Population sizes (N) and mutation rates (μ) to balance theta distribution\n",
    "MIN_N = 50_000                # Increased minimum N\n",
    "MAX_N = 200_000               # Increased maximum N\n",
    "N_values = np.linspace(MIN_N, MAX_N, num=20, dtype=int)  # 20 values from 50,000 to 200,000\n",
    "\n",
    "MIN_MU = 1e-8\n",
    "MAX_MU = 1e-6                 # Increased maximum μ\n",
    "mu_values = np.logspace(np.log10(MIN_MU), np.log10(MAX_MU), num=20)  # 20 values on log scale\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Simulation & Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     genotypes_list\u001b[38;5;241m.\u001b[39mappend(genotypes)\n\u001b[1;32m     38\u001b[0m     allele_frequencies\u001b[38;5;241m.\u001b[39mappend(frequency)\n\u001b[0;32m---> 39\u001b[0m     positions\u001b[38;5;241m.\u001b[39mappend(\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msite\u001b[49m\u001b[38;5;241m.\u001b[39mposition)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Convert lists to arrays\u001b[39;00m\n\u001b[1;32m     42\u001b[0m allele_frequencies \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(allele_frequencies)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tskit/genotypes.py:141\u001b[0m, in \u001b[0;36mVariant.site\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03mThe Site object for the site at which this variant has been decoded.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_decoded()\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_sequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ll_variant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msite_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tskit/trees.py:6073\u001b[0m, in \u001b[0;36mTreeSequence.site\u001b[0;34m(self, id_, position)\u001b[0m\n\u001b[1;32m   6065\u001b[0m pos, ancestral_state, ll_mutations, _, metadata \u001b[38;5;241m=\u001b[39m ll_site\n\u001b[1;32m   6066\u001b[0m mutations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutation(mut_id) \u001b[38;5;28;01mfor\u001b[39;00m mut_id \u001b[38;5;129;01min\u001b[39;00m ll_mutations]\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Site(\n\u001b[1;32m   6068\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m   6069\u001b[0m     position\u001b[38;5;241m=\u001b[39mpos,\n\u001b[1;32m   6070\u001b[0m     ancestral_state\u001b[38;5;241m=\u001b[39mancestral_state,\n\u001b[1;32m   6071\u001b[0m     mutations\u001b[38;5;241m=\u001b[39mmutations,\n\u001b[1;32m   6072\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m-> 6073\u001b[0m     metadata_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_metadata_schemas\u001b[49m\u001b[38;5;241m.\u001b[39msite\u001b[38;5;241m.\u001b[39mdecode_row,\n\u001b[1;32m   6074\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tskit/trees.py:4317\u001b[0m, in \u001b[0;36mTreeSequence.table_metadata_schemas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;124;03m    Returns the number of sample nodes in this tree sequence. This is also the\u001b[39;00m\n\u001b[1;32m   4310\u001b[0m \u001b[38;5;124;03m    number of sample nodes in each tree.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4313\u001b[0m \u001b[38;5;124;03m    :rtype: int\u001b[39;00m\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ll_tree_sequence\u001b[38;5;241m.\u001b[39mget_num_samples()\n\u001b[0;32m-> 4317\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   4318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable_metadata_schemas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TableMetadataSchemas:\n\u001b[1;32m   4319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4320\u001b[0m \u001b[38;5;124;03m    The set of metadata schemas for the tables in this tree sequence.\u001b[39;00m\n\u001b[1;32m   4321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table_metadata_schemas\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store inputs and labels\n",
    "inputs_basic = []\n",
    "inputs_derived = []\n",
    "inputs_combined = []\n",
    "labels = []\n",
    "population_sizes = []\n",
    "mutation_rates = []\n",
    "\n",
    "for i in range(NUM_DATASETS):\n",
    "    # Randomly select population size N and mutation rate μ\n",
    "    N = random.choice(N_values)\n",
    "    mu = random.choice(mu_values)\n",
    "    theta = 4 * N * mu  # θ = 4Nμ\n",
    "\n",
    "    # Simulate ancestral history\n",
    "    ts = msprime.sim_ancestry(\n",
    "        samples=NUM_SAMPLES_PER_SIM,\n",
    "        recombination_rate=0,\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        population_size=N,\n",
    "        random_seed=random.randint(1, 1e6)\n",
    "    )\n",
    "\n",
    "    # Simulate mutations\n",
    "    mts = msprime.sim_mutations(ts, rate=mu, random_seed=random.randint(1, 1e6))\n",
    "\n",
    "    # Collect genotype data and allele frequencies\n",
    "    genotypes_list = []\n",
    "    allele_frequencies = []\n",
    "    positions = []\n",
    "\n",
    "    for variant in mts.variants():\n",
    "        genotypes = variant.genotypes  # Array of 0s and 1s\n",
    "        derived_allele_count = np.count_nonzero(genotypes)  # Number of '1's\n",
    "        frequency = derived_allele_count / mts.num_samples  # Allele frequency\n",
    "\n",
    "        genotypes_list.append(genotypes)\n",
    "        allele_frequencies.append(frequency)\n",
    "        positions.append(variant.site.position)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    allele_frequencies = np.array(allele_frequencies)\n",
    "    positions = np.array(positions)\n",
    "\n",
    "    # Proceed only if there are variants\n",
    "    if len(allele_frequencies) == 0:\n",
    "        continue  # Skip this iteration if no variants are present\n",
    "\n",
    "    genotypes_array = np.vstack(genotypes_list)  # Shape: (num_variants, NUM_SAMPLES)\n",
    "    allele_counts = genotypes_array.sum(axis=1)\n",
    "\n",
    "    # Compute Basic Statistics\n",
    "    S = len(allele_frequencies)  # Number of segregating sites\n",
    "    pi = 2 * np.sum(allele_frequencies * (1 - allele_frequencies))\n",
    "    singleton_count = np.sum(allele_counts == 1)\n",
    "    allele_freq_variance = np.var(allele_frequencies)\n",
    "    allele_freq_skewness = skew(allele_frequencies)\n",
    "    allele_freq_kurtosis = kurtosis(allele_frequencies)\n",
    "    rare_variant_threshold = 0.05\n",
    "    rare_variant_proportion = np.sum(allele_frequencies <= rare_variant_threshold) / S\n",
    "    mean_inter_snp_distance = np.mean(np.diff(np.sort(positions))) if S > 1 else 0.0\n",
    "    var_inter_snp_distance = np.var(np.diff(np.sort(positions))) if S > 1 else 0.0\n",
    "\n",
    "    # Prepare basic statistics feature vector\n",
    "    features_basic = np.array([\n",
    "        S,\n",
    "        pi,\n",
    "        singleton_count,\n",
    "        allele_freq_variance,\n",
    "        allele_freq_skewness,\n",
    "        allele_freq_kurtosis,\n",
    "        rare_variant_proportion,\n",
    "        mean_inter_snp_distance,\n",
    "        var_inter_snp_distance\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Compute Derived Statistics\n",
    "    n = mts.num_samples\n",
    "    a1 = np.sum(1.0 / np.arange(1, n))\n",
    "    a2 = np.sum(1.0 / (np.arange(1, n) ** 2))\n",
    "    b1 = (n + 1) / (3 * (n - 1))\n",
    "    b2 = 2 * (n ** 2 + n + 3) / (9 * n * (n - 1))\n",
    "    c1 = b1 - (1 / a1)\n",
    "    c2 = b2 - ((n + 2) / (a1 * n)) + (a2 / (a1 ** 2))\n",
    "    e1 = c1 / a1\n",
    "    e2 = c2 / (a1 ** 2 + a2)\n",
    "    variance_tajd = e1 * S + e2 * S * (S - 1)\n",
    "    tajimas_d = (pi - (S / a1)) / np.sqrt(variance_tajd) if variance_tajd > 0 else 0.0\n",
    "\n",
    "    # Prepare derived statistics feature vector\n",
    "    features_derived = np.array([\n",
    "        tajimas_d\n",
    "        # Add other derived statistics if computed\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Combine features\n",
    "    features_combined = np.concatenate([features_basic, features_derived])\n",
    "\n",
    "    # Append features and labels\n",
    "    inputs_basic.append(features_basic)\n",
    "    inputs_derived.append(features_derived)\n",
    "    inputs_combined.append(features_combined)\n",
    "    labels.append(theta)\n",
    "    population_sizes.append(N)\n",
    "    mutation_rates.append(mu)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{NUM_DATASETS} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert lists to Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert lists to arrays\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs_basic_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(inputs_basic, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      3\u001b[0m inputs_derived_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inputs_derived, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m inputs_combined_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inputs_combined, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert lists to arrays\n",
    "inputs_basic_array = np.array(inputs_basic, dtype=np.float32)\n",
    "inputs_derived_array = np.array(inputs_derived, dtype=np.float32)\n",
    "inputs_combined_array = np.array(inputs_combined, dtype=np.float32)\n",
    "labels_array = np.array(labels, dtype=np.float32)\n",
    "population_sizes_array = np.array(population_sizes, dtype=np.int32)\n",
    "mutation_rates_array = np.array(mutation_rates, dtype=np.float32)\n",
    "\n",
    "# Scale Theta values by a scaling factor\n",
    "scaling_factor = 1e6  # Scaling theta to bring values into a larger range\n",
    "labels_array_scaled = labels_array * scaling_factor\n",
    "\n",
    "# Apply Box-Cox transformation to scaled theta values\n",
    "labels_array_transformed, lambda_ = stats.boxcox(labels_array_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split -- Training, Testing, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of indices representing each sample\n",
    "indices = np.arange(len(labels_array_transformed))\n",
    "\n",
    "# Split indices into training+validation and test sets\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    indices, test_size=0.15, random_state=42)\n",
    "\n",
    "# Further split training+validation indices into training and validation sets\n",
    "train_indices, val_indices = train_test_split(\n",
    "    train_val_indices, test_size=0.15 / 0.85, random_state=42)\n",
    "\n",
    "# Split data for basic statistics using indices\n",
    "X_basic_train = inputs_basic_array[train_indices]\n",
    "X_basic_val = inputs_basic_array[val_indices]\n",
    "X_basic_test = inputs_basic_array[test_indices]\n",
    "\n",
    "# Split data for derived statistics using indices\n",
    "X_derived_train = inputs_derived_array[train_indices]\n",
    "X_derived_val = inputs_derived_array[val_indices]\n",
    "X_derived_test = inputs_derived_array[test_indices]\n",
    "\n",
    "# Split data for combined features using indices\n",
    "X_combined_train = inputs_combined_array[train_indices]\n",
    "X_combined_val = inputs_combined_array[val_indices]\n",
    "X_combined_test = inputs_combined_array[test_indices]\n",
    "\n",
    "# Split transformed labels using indices\n",
    "y_train = labels_array_transformed[train_indices]\n",
    "y_val = labels_array_transformed[val_indices]\n",
    "y_test = labels_array_transformed[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Scalers on Training Data and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "scaler_basic = StandardScaler()\n",
    "scaler_derived = StandardScaler()\n",
    "scaler_combined = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data\n",
    "scaler_basic.fit(X_basic_train)\n",
    "scaler_derived.fit(X_derived_train)\n",
    "scaler_combined.fit(X_combined_train)\n",
    "\n",
    "# Transform training, validation, and test data\n",
    "X_basic_train_scaled = scaler_basic.transform(X_basic_train)\n",
    "X_basic_val_scaled = scaler_basic.transform(X_basic_val)\n",
    "X_basic_test_scaled = scaler_basic.transform(X_basic_test)\n",
    "\n",
    "X_derived_train_scaled = scaler_derived.transform(X_derived_train)\n",
    "X_derived_val_scaled = scaler_derived.transform(X_derived_val)\n",
    "X_derived_test_scaled = scaler_derived.transform(X_derived_test)\n",
    "\n",
    "X_combined_train_scaled = scaler_combined.transform(X_combined_train)\n",
    "X_combined_val_scaled = scaler_combined.transform(X_combined_val)\n",
    "X_combined_test_scaled = scaler_combined.transform(X_combined_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature names\n",
    "feature_names_basic = [\n",
    "    'S',  # Number of segregating sites\n",
    "    'pi',  # Nucleotide diversity\n",
    "    'singleton_count',\n",
    "    'allele_freq_variance',\n",
    "    'allele_freq_skewness',\n",
    "    'allele_freq_kurtosis',\n",
    "    'rare_variant_proportion',\n",
    "    'mean_inter_snp_distance',\n",
    "    'var_inter_snp_distance'\n",
    "]\n",
    "\n",
    "feature_names_derived = [\n",
    "    'tajimas_d'\n",
    "    # Add other derived statistic names if included\n",
    "]\n",
    "\n",
    "# Combine feature names\n",
    "feature_names_combined = feature_names_basic + feature_names_derived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Theta Distribution and Feature statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the distribution of transformed theta values\n",
    "plt.hist(labels_array_transformed, bins=50, edgecolor='k')\n",
    "plt.xlabel('Box-Cox Transformed Theta')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Box-Cox Transformed Theta Values')\n",
    "plt.show()\n",
    "\n",
    "# Print statistical summaries of transformed theta\n",
    "print(f\"Transformed Theta Min: {labels_array_transformed.min()}\")\n",
    "print(f\"Transformed Theta Max: {labels_array_transformed.max()}\")\n",
    "print(f\"Transformed Theta Mean: {labels_array_transformed.mean()}\")\n",
    "print(f\"Transformed Theta Std Dev: {labels_array_transformed.std()}\")\n",
    "print(f\"Box-Cox Lambda: {lambda_}\")\n",
    "\n",
    "# Create DataFrame for combined features\n",
    "df_features = pd.DataFrame(inputs_combined_array, columns=feature_names_combined)\n",
    "df_features['theta_transformed'] = labels_array_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct and Visualizing Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix with transformed theta\n",
    "corr_matrix_transformed = df_features.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr_matrix_transformed, annot=True, fmt=\".2f\", cmap=cmap, center=0, square=True, linewidths=.5)\n",
    "plt.title('Correlation Matrix of Features and Transformed Theta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    # Use a smaller learning rate\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "train_dataset_basic = tf.data.Dataset.from_tensor_slices((X_basic_train_scaled, y_train))\n",
    "train_dataset_basic = train_dataset_basic.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset_basic = tf.data.Dataset.from_tensor_slices((X_basic_val_scaled, y_val))\n",
    "val_dataset_basic = val_dataset_basic.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset_basic = tf.data.Dataset.from_tensor_slices((X_basic_test_scaled, y_test))\n",
    "test_dataset_basic = test_dataset_basic.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the model\n",
    "model_basic = create_model(input_dim=X_basic_train_scaled.shape[1])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history_basic = model_basic.fit(\n",
    "    train_dataset_basic,\n",
    "    epochs=200,\n",
    "    validation_data=val_dataset_basic,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_basic_transformed, test_mae_basic_transformed = model_basic.evaluate(test_dataset_basic)\n",
    "print(f\"Basic Statistics Model - Test Loss (MSE): {test_loss_basic_transformed:.4f}\")\n",
    "print(f\"Basic Statistics Model - Test MAE: {test_mae_basic_transformed:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_basic_transformed = model_basic.predict(test_dataset_basic).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Derived Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "train_dataset_derived = tf.data.Dataset.from_tensor_slices((X_derived_train_scaled, y_train))\n",
    "train_dataset_derived = train_dataset_derived.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset_derived = tf.data.Dataset.from_tensor_slices((X_derived_val_scaled, y_val))\n",
    "val_dataset_derived = val_dataset_derived.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset_derived = tf.data.Dataset.from_tensor_slices((X_derived_test_scaled, y_test))\n",
    "test_dataset_derived = test_dataset_derived.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the model\n",
    "model_derived = create_model(input_dim=X_derived_train_scaled.shape[1])\n",
    "\n",
    "# Train the model\n",
    "history_derived = model_derived.fit(\n",
    "    train_dataset_derived,\n",
    "    epochs=200,\n",
    "    validation_data=val_dataset_derived,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_derived_transformed, test_mae_derived_transformed = model_derived.evaluate(test_dataset_derived)\n",
    "print(f\"Derived Statistics Model - Test Loss (MSE): {test_loss_derived_transformed:.4f}\")\n",
    "print(f\"Derived Statistics Model - Test MAE: {test_mae_derived_transformed:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_derived_transformed = model_derived.predict(test_dataset_derived).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "train_dataset_combined = tf.data.Dataset.from_tensor_slices((X_combined_train_scaled, y_train))\n",
    "train_dataset_combined = train_dataset_combined.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset_combined = tf.data.Dataset.from_tensor_slices((X_combined_val_scaled, y_val))\n",
    "val_dataset_combined = val_dataset_combined.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset_combined = tf.data.Dataset.from_tensor_slices((X_combined_test_scaled, y_test))\n",
    "test_dataset_combined = test_dataset_combined.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the model\n",
    "model_combined = create_model(input_dim=X_combined_train_scaled.shape[1])\n",
    "\n",
    "# Train the model\n",
    "history_combined = model_combined.fit(\n",
    "    train_dataset_combined,\n",
    "    epochs=200,\n",
    "    validation_data=val_dataset_combined,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_combined_transformed, test_mae_combined_transformed = model_combined.evaluate(test_dataset_combined)\n",
    "print(f\"Combined Features Model - Test Loss (MSE): {test_loss_combined_transformed:.4f}\")\n",
    "print(f\"Combined Features Model - Test MAE: {test_mae_combined_transformed:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_combined_transformed = model_combined.predict(test_dataset_combined).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Back Theta(Predictions) and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Box-Cox transformation to get predictions and true values in original scale\n",
    "y_test_original = stats.inv_boxcox(y_test, lambda_)\n",
    "\n",
    "y_pred_basic = stats.inv_boxcox(y_pred_basic_transformed, lambda_)\n",
    "y_pred_derived = stats.inv_boxcox(y_pred_derived_transformed, lambda_)\n",
    "y_pred_combined = stats.inv_boxcox(y_pred_combined_transformed, lambda_)\n",
    "\n",
    "# Compute Test MSE, MAE, R², and MAPE with original theta scale\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Basic Statistics Model\n",
    "test_mse_basic = mean_squared_error(y_test_original, y_pred_basic)\n",
    "test_mae_basic = mean_absolute_error(y_test_original, y_pred_basic)\n",
    "r2_basic = r2_score(y_test_original, y_pred_basic)\n",
    "mape_basic = mean_absolute_percentage_error(y_test_original, y_pred_basic)\n",
    "\n",
    "# Derived Statistics Model\n",
    "test_mse_derived = mean_squared_error(y_test_original, y_pred_derived)\n",
    "test_mae_derived = mean_absolute_error(y_test_original, y_pred_derived)\n",
    "r2_derived = r2_score(y_test_original, y_pred_derived)\n",
    "mape_derived = mean_absolute_percentage_error(y_test_original, y_pred_derived)\n",
    "\n",
    "# Combined Features Model\n",
    "test_mse_combined = mean_squared_error(y_test_original, y_pred_combined)\n",
    "test_mae_combined = mean_absolute_error(y_test_original, y_pred_combined)\n",
    "r2_combined = r2_score(y_test_original, y_pred_combined)\n",
    "mape_combined = mean_absolute_percentage_error(y_test_original, y_pred_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results into a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Basic Statistics', 'Derived Statistics', 'Combined Features'],\n",
    "    'Test MSE': [test_mse_basic, test_mse_derived, test_mse_combined],\n",
    "    'Test MAE': [test_mae_basic, test_mae_derived, test_mae_combined],\n",
    "    'Test R2': [r2_basic, r2_derived, r2_combined],\n",
    "    'Test MAPE': [mape_basic, mape_derived, mape_combined]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction statistics\n",
    "predictions = [y_pred_basic, y_pred_derived, y_pred_combined]\n",
    "models = ['Basic Statistics', 'Derived Statistics', 'Combined Features']\n",
    "\n",
    "for model_name, y_pred in zip(models, predictions):\n",
    "    print(f\"\\n{model_name} Predictions:\")\n",
    "    print(f\"Min: {y_pred.min()}\")\n",
    "    print(f\"Max: {y_pred.max()}\")\n",
    "    print(f\"Mean: {y_pred.mean()}\")\n",
    "    print(f\"Std Dev: {y_pred.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: Predicted vs True Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. true theta values for each model\n",
    "models = ['Basic Statistics', 'Derived Statistics', 'Combined Features']\n",
    "predictions = [y_pred_basic, y_pred_derived, y_pred_combined]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "for i, (model_name, y_pred) in enumerate(zip(models, predictions)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(y_test_original, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--')\n",
    "    plt.xlabel('True Theta Values')\n",
    "    plt.ylabel('Predicted Theta Values')\n",
    "    plt.title(f'Predicted vs. True Theta ({model_name})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss for each model\n",
    "histories = [history_basic, history_derived, history_combined]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "for i, (model_name, history) in enumerate(zip(models, histories)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Model Loss ({model_name})')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Model Comparison Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Stats Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for Test MSE, MAE, R², and MAPE\n",
    "metrics = ['Test MSE', 'Test MAE', 'Test R2', 'Test MAPE']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x='Model', y=metric, data=results)\n",
    "    plt.title(f'Model Comparison - {metric}')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Model')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
